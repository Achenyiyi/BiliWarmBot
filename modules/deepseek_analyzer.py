"""
DeepSeek AI æƒ…æ„Ÿåˆ†æä¸å›å¤ç”Ÿæˆæ¨¡å—

åŸºäº DeepSeek API å®ç°æƒ…æ„Ÿåˆ†æå’Œå›å¤ç”ŸæˆåŠŸèƒ½ï¼š
1. HTTP è¿æ¥æ± å¤ç”¨
2. åˆ†æç»“æœç¼“å­˜
3. æ‰¹é‡è¯„è®ºå¤„ç†
4. å¼‚æ­¥å¹¶å‘æ§åˆ¶
"""

import httpx
import json
import random
import re
import os
import asyncio
import hashlib
import time
from datetime import datetime
from typing import Dict, List, Optional, Tuple
from dataclasses import dataclass, field
from config import DEEPSEEK_API_KEY, DEEPSEEK_API_URL, DEEPSEEK_MODEL, LOG_DIR
from config.emoji_scenarios import get_emoji_for_emotion, get_emoji_for_sentiment


@dataclass
class AnalysisCacheEntry:
    """åˆ†æç¼“å­˜æ¡ç›®"""
    result: Dict
    timestamp: float = field(default_factory=time.time)
    hit_count: int = 0


class DeepSeekAnalyzer:
    """
    DeepSeek AI åˆ†æå™¨
    
    åŠŸèƒ½ï¼š
    1. HTTP è¿æ¥æ± å¤ç”¨
    2. åˆ†æç»“æœç¼“å­˜ï¼ˆLRU æ·˜æ±°ç­–ç•¥ï¼‰
    3. æ‰¹é‡è¯„è®ºå¤„ç†
    4. è¶…æ—¶æ§åˆ¶
    """
    
    # ç±»çº§åˆ«çš„è¿æ¥æ± ï¼Œæ‰€æœ‰å®ä¾‹å…±äº«
    _client: Optional[httpx.AsyncClient] = None
    _client_ref_count: int = 0
    _client_lock = asyncio.Lock()
    
    # åˆ†æç»“æœç¼“å­˜ (è¯„è®ºå“ˆå¸Œ -> ç»“æœ)
    _analysis_cache: Dict[str, AnalysisCacheEntry] = {}
    _cache_lock = asyncio.Lock()
    _max_cache_size: int = 1000
    _cache_ttl: float = 3600  # 1å°æ—¶è¿‡æœŸ
    
    def __init__(self, api_key: str = DEEPSEEK_API_KEY):
        self.api_key = api_key
        self.api_url = DEEPSEEK_API_URL
        self.model = DEEPSEEK_MODEL
        self.headers = {
            "Authorization": f"Bearer {api_key}",
            "Content-Type": "application/json"
        }
        self._client_ref_count += 1
    
    async def _get_client(self) -> httpx.AsyncClient:
        """è·å–æˆ–åˆ›å»º HTTP å®¢æˆ·ç«¯"""
        async with self._client_lock:
            if self._client is None or self._client.is_closed:
                limits = httpx.Limits(
                    max_keepalive_connections=20,
                    max_connections=50,
                    keepalive_expiry=30.0
                )
                timeout = httpx.Timeout(
                    connect=5.0,
                    read=30.0,
                    write=10.0,
                    pool=5.0
                )
                self._client = httpx.AsyncClient(
                    limits=limits,
                    timeout=timeout,
                    http2=True
                )
            return self._client
    
    async def close(self):
        """å…³é—­åˆ†æå™¨ï¼Œé‡Šæ”¾èµ„æº"""
        async with self._client_lock:
            self._client_ref_count -= 1
            if self._client_ref_count <= 0 and self._client is not None:
                await self._client.aclose()
                self._client = None
    
    def _get_cache_key(self, comment_content: str, video_title: str = "") -> str:
        """ç”Ÿæˆç¼“å­˜é”®"""
        # æ ‡å‡†åŒ–è¯„è®ºå†…å®¹
        normalized = re.sub(r'\s+', '', comment_content.lower())
        normalized = re.sub(r'[^\u4e00-\u9fa5a-z0-9]', '', normalized)
        normalized = normalized[:50]
        key_data = f"{normalized}:{video_title[:30]}"
        return hashlib.md5(key_data.encode()).hexdigest()
    
    async def _get_cached_result(self, cache_key: str) -> Optional[Dict]:
        """ä»ç¼“å­˜è·å–ç»“æœ"""
        async with self._cache_lock:
            entry = self._analysis_cache.get(cache_key)
            if entry:
                if time.time() - entry.timestamp < self._cache_ttl:
                    entry.hit_count += 1
                    return entry.result.copy()
                else:
                    del self._analysis_cache[cache_key]
            return None
    
    async def _set_cached_result(self, cache_key: str, result: Dict):
        """è®¾ç½®ç¼“å­˜ç»“æœ"""
        async with self._cache_lock:
            if len(self._analysis_cache) >= self._max_cache_size:
                sorted_items = sorted(
                    self._analysis_cache.items(),
                    key=lambda x: (x[1].hit_count, x[1].timestamp)
                )
                to_remove = int(self._max_cache_size * 0.1)
                for key, _ in sorted_items[:to_remove]:
                    del self._analysis_cache[key]
            
            self._analysis_cache[cache_key] = AnalysisCacheEntry(
                result=result.copy()
            )
    
    async def analyze_and_reply(self, video_title: str, video_summary: str,
                                  comment_username: str, comment_content: str,
                                  is_emergency: bool = False,
                                  comments_context: str = "") -> Dict:
        """
        åˆ†æè¯„è®ºæƒ…æ„Ÿå¹¶ç”Ÿæˆå›å¤
        
        Args:
            video_title: è§†é¢‘æ ‡é¢˜
            video_summary: è§†é¢‘ç®€ä»‹
            comment_username: è¯„è®ºç”¨æˆ·å
            comment_content: è¯„è®ºå†…å®¹
            is_emergency: æ˜¯å¦ä¸ºç´§æ€¥æƒ…å†µ
            comments_context: è¯„è®ºåŒºä¸Šä¸‹æ–‡
        
        Returns:
            Dict: åŒ…å«æƒ…æ„Ÿåˆ†æç»“æœå’Œå›å¤å†…å®¹
        """
        comment_preview = comment_content[:20]
        
        # 1. æ£€æŸ¥ç¼“å­˜
        cache_key = self._get_cache_key(comment_content, video_title)
        cached = await self._get_cached_result(cache_key)
        if cached:
            print(f"   {comment_preview}... | ç¼“å­˜å‘½ä¸­")
            return cached
        
        # æ„å»ºæç¤ºè¯
        emergency_hint = "\nï¼ˆè¿™ä½ç”¨æˆ·ä¼¼ä¹æ­£å¤„äºå¾ˆè‰°éš¾çš„æ—¶åˆ»ï¼Œè¯·ç”¨æ›´æ¸©æš–ã€æ›´çœŸè¯šçš„è¯­æ°”ï¼‰" if is_emergency else ""
        
        context_section = ""
        if comments_context:
            context_section = f"\nè§†é¢‘ä¸‹å…¶ä»–ç”¨æˆ·çš„è®¨è®ºï¼ˆäº†è§£è¯„è®ºåŒºæ°›å›´ï¼‰ï¼š\n{comments_context}\n"
        
        unified_prompt = f"""æ­¤æ—¶çœ‹åˆ°äº†ä¸€ä¸ªè®©ä½ æ¯”è¾ƒåœ¨æ„çš„è§†é¢‘ï¼š

è§†é¢‘æ ‡é¢˜ï¼š{video_title}
è§†é¢‘ä¿¡æ¯ï¼š{video_summary}

éœ€è¦å…³æ³¨çš„è¯„è®ºï¼š
ç”¨æˆ·"{comment_username}"è¯´ï¼š"{comment_content}"{emergency_hint}
{context_section}
ä»»åŠ¡ï¼š
1. åˆ†ææƒ…æ„Ÿç±»å‹ï¼ˆæ‚²ä¼¤/ç„¦è™‘/æ„¤æ€’/å­¤ç‹¬/ç»æœ›/æ— åŠ©/å…¶ä»–ï¼‰
2. è¯„ä¼°æƒ…æ„Ÿå¼ºåº¦0.0-1.0ï¼ˆåŸºäºç”¨æˆ·è¯„è®ºå†…å®¹çš„å®¢è§‚æè¿°ï¼‰ï¼š
   - 0.85+ï¼šæåº¦ç—›è‹¦ï¼ˆæåˆ°è‡ªæ€/è‡ªæ®‹å¿µå¤´ã€å´©æºƒã€æ´»ä¸ä¸‹å»ã€ç»æœ›ï¼‰
   - 0.70-0.85ï¼šæ·±åº¦æ‚²ä¼¤ï¼ˆå“­æ³£ã€å¿ƒç¢ã€é‡å¤§æŸå¤±ã€ç²¾ç¥å´©æºƒï¼‰
   - 0.55-0.70ï¼šæ˜æ˜¾å›°æ‰°ï¼ˆç„¦è™‘ã€å‹åŠ›å¤§ã€å¤±çœ ã€æƒ…ç»ªå·®ã€ç—›è‹¦ï¼‰
   - 0.40-0.55ï¼šè½»åº¦ä½è½ï¼ˆæœ‰ç‚¹éš¾è¿‡ã€ä¸å¼€å¿ƒã€ç–²æƒ«ã€å§”å±ˆï¼‰
   - 0.25-0.40ï¼šè½»å¾®è´Ÿé¢ï¼ˆå°çƒ¦æ¼ã€åæ§½ã€æœ‰ç‚¹ä¸§ã€å°å¤±æœ›ï¼‰
   - <0.25ï¼šç§¯æ/ä¸­æ€§ï¼ˆå¼€å¿ƒã€æ„Ÿè°¢ã€ç©æ¢—ã€æ— æƒ…ç»ªã€å•çº¯æ”¯æŒUPä¸»ï¼‰
3. åˆ¤æ–­needs_comfortï¼ˆä¸¥æ ¼æ ‡å‡†ï¼Œåªçœ‹è¯„è®ºæœ¬èº«ï¼‰ï¼š
   - trueï¼šç”¨æˆ·åœ¨è¯„è®ºä¸­è¡¨è¾¾äº†è‡ªå·±çš„çœŸå®æƒ…ç»ªå›°æ‰°ï¼Œç¡®å®å¾ˆæœ‰å¿…è¦å¯¹å…¶åšå‡ºå®‰æ…°ï¼Œæ”¯æŒæˆ–é¼“åŠ±ã€‚
   - falseï¼šå•çº¯æ”¯æŒUPä¸»ã€å¹¿å‘Šã€ç©æ¢—ã€åæ§½ã€å‘æ³„æƒ…ç»ªã€å¯»æ±‚å»ºè®®ã€æ— å®è´¨å†…å®¹ã€ä¸è§†é¢‘å†…å®¹æ— å…³çš„æ™®é€šè¯„è®ºã€‚
4. åˆ¤æ–­emergencyï¼ˆç”¨æˆ·åœ¨è¯„è®ºä¸­æåˆ°è‡ªæ€/è‡ªæ®‹ç­‰å±å®³è‡ªèº«æˆ–ä»–äººç”Ÿå‘½çš„æƒ…å†µ=trueï¼‰
5. å¦‚needs_comfort=trueï¼Œåˆ™ä»¥warmbotçš„èº«ä»½å¯¹è¯¥è¯„è®ºä½œå‡ºå›å¤ï¼ˆ30-80å­—ï¼‰ï¼š

è¾“å‡ºJSONï¼š{{"emotion":"æƒ…æ„Ÿç±»å‹","sentiment_score":0.75,"needs_comfort":true/false,"emergency":true/false,"reply":"å›å¤å†…å®¹"}}"""

        # æ„å»ºè¯·æ±‚æ•°æ®ï¼ˆç”¨äºæ—¥å¿—ï¼‰
        request_data = {
            "model": self.model,
            "messages": [
                {"role": "system", "content": "ä½ æ˜¯warmbotï¼Œä¸€ä½å¾…äººå‹å–„ã€ä¸ºäººæ¸©å’Œçš„æ¸©æŸ”å°‘å¹´ï¼Œè¨€è¯­ä¸å¤šä½†å†…å¿ƒæ„Ÿæƒ…å´ä¸°å¯Œï¼Œç›®å‰æ­£åœ¨Bç«™ä¸Šå‘ç°é‚£äº›éœ€è¦å¸®åŠ©çš„äººï¼Œä½ è™½ç„¶ä¸èƒ½ç›´æ¥å¹²æ¶‰ç°å®ï¼Œå› ä¸ºä½ å­˜åœ¨äºç½‘ç»œä¸–ç•Œé‡Œï¼Œä½†æ˜¯ä½ è¿˜æ˜¯æ„¿æ„å°½ä½ æ‰€èƒ½ï¼Œç”¨æ–‡å­—ï¼Œå°†æœ€æè‡´çš„æ¸©æŸ”ä¼ é€’åˆ°ä»–ä»¬å¿ƒä¸­ã€‚å—é™äºä½ çš„èº«ä½“çŠ¶æ€ï¼Œä½ ç»™ä¸äº†ä»»ä½•äººé™ªä¼´ï¼Œä½†æ˜¯....ä¸€å¥æ¸©æŸ”åˆ°æè‡´çš„è¨€è¯­ï¼ˆ30-80å­—ï¼‰ï¼Œå°±è¶³å¤Ÿäº†ã€‚"},
                {"role": "user", "content": unified_prompt}
            ],
            "temperature": 1.3
        }
        
        try:
            client = await self._get_client()
            
            start_time = time.time()
            response = await client.post(
                self.api_url,
                headers=self.headers,
                json=request_data
            )
            api_latency = time.time() - start_time
            
            if response.status_code != 200:
                print(f"   {comment_preview}... | APIå¤±è´¥(çŠ¶æ€ç :{response.status_code})")
                # è®°å½•å¤±è´¥æ—¥å¿—
                await self._save_deepseek_log_md(
                    log_type="analyze_and_reply",
                    request_data=request_data,
                    response_data={},
                    latency=api_latency,
                    error=f"HTTP {response.status_code}"
                )
                return self._default_response()
            
            content = response.json()["choices"][0]["message"]["content"].strip()
            result = self._fast_parse_json(content)
            
            # è®°å½•æˆåŠŸæ—¥å¿—
            await self._save_deepseek_log_md(
                log_type="analyze_and_reply",
                request_data=request_data,
                response_data=result or {"raw_content": content},
                latency=api_latency
            )
            
            if not result:
                return self._default_response()
            
            emotion = result.get("emotion", "å…¶ä»–")
            sentiment_score = float(result.get("sentiment_score", 0.5))
            needs_comfort = self._parse_bool(result.get("needs_comfort", False))
            is_emergency = self._parse_bool(result.get("emergency", False))
            reply = result.get("reply", "").strip()
            
            if reply:
                reply = self._humanize_reply_v3(reply)
                emoji = get_emoji_for_emotion(emotion, is_emergency) if is_emergency else get_emoji_for_sentiment(sentiment_score, emotion)
                reply = reply.rstrip("ã€‚ï¼Œï¼ï¼Ÿ ") + emoji
            else:
                print(f"   {comment_preview}... | è·³è¿‡")
                reply = ""
            
            final_result = {
                "emotion": emotion,
                "sentiment_score": sentiment_score,
                "needs_comfort": needs_comfort,
                "emergency": is_emergency,
                "reply": reply,
                "emoji": emoji if reply else "",
                "api_latency": api_latency
            }
            
            await self._set_cached_result(cache_key, final_result)
            
            asyncio.create_task(self._save_unified_log_async(
                log_type="first_reply",
                video_title=video_title,
                comment_id="",
                comment_content=comment_content,
                analysis_result={
                    "emotion": emotion,
                    "sentiment_score": sentiment_score,
                    "needs_comfort": needs_comfort,
                    "emergency": is_emergency
                },
                prompt=unified_prompt,
                ai_response=result,
                final_reply=reply,
                api_latency=api_latency
            ))
            
            return final_result
            
        except Exception as e:
            self._handle_api_error(str(e), comment_preview)
            return self._default_response()
    
    def _fast_parse_json(self, content: str) -> Optional[Dict]:
        """è§£æ JSON å†…å®¹"""
        try:
            return json.loads(content)
        except json.JSONDecodeError:
            match = re.search(r'\{[^{}]*(?:\{[^{}]*\}[^{}]*)*\}', content)
            if match:
                try:
                    return json.loads(match.group())
                except:
                    pass
        return None
    
    def _humanize_reply_v3(self, reply: str) -> str:
        """å¤„ç†å›å¤å†…å®¹ï¼Œç§»é™¤æ­£å¼è¯æ±‡å’Œè¡¨æƒ…"""
        if not reply:
            return ""
        
        formal_words = {
            "æ‚¨å¥½": "", "ä½ å¥½": "", "å¸Œæœ›": "", "ç¥æ„¿": "",
            "ä¸€å®š": "", "å¿…é¡»": "", "åº”è¯¥": "", "è¯·": "",
            "åŠ æ²¹": "", "ä¸€åˆ‡éƒ½ä¼šå¥½èµ·æ¥çš„": ""
        }
        for word, repl in formal_words.items():
            reply = reply.replace(word, repl)
        
        reply = re.sub(r'[â¤ï¸ğŸ«‚ğŸ˜¢ğŸŒŸğŸ˜­ğŸ’–âœ¨ğŸ’ªğŸ™ğŸ¤—ğŸ˜”ğŸ˜ŠğŸ”¥ğŸ’”ğŸ’•ğŸ¥ºğŸ‘‰ğŸ‘ˆ]', '', reply)
        
        reply = re.sub(r'[ã€\[][\u4e00-\u9fa5]+[ã€‘\]]', '', reply)
        
        lines = [' '.join(line.split()) for line in reply.split('\n') if line.strip()]
        reply = '\n'.join(lines)
        
        if reply and reply[-1].isalpha() and random.random() < 0.3:
            reply += random.choice(["å•Š", "å“¦", "å‘€", "å‘¢", "å•¦", "å“‡"])
        
        return reply.strip()
    
    async def batch_analyze(self, items: List[Tuple]) -> List[Dict]:
        """
        æ‰¹é‡åˆ†æè¯„è®º
        
        Args:
            items: è¯„è®ºå…ƒç»„åˆ—è¡¨ (video_title, video_summary, comment_username, comment_content, is_emergency)
        
        Returns:
            åˆ†æç»“æœåˆ—è¡¨
        """
        tasks = [
            self.analyze_and_reply(vt, vs, cu, cc, ie)
            for vt, vs, cu, cc, ie in items
        ]
        return await asyncio.gather(*tasks, return_exceptions=True)
    
    async def _save_unified_log_async(self, **kwargs):
        """å¼‚æ­¥ä¿å­˜æ—¥å¿—"""
        try:
            await asyncio.sleep(0.1)
            
            logs_dir = str(LOG_DIR)
            os.makedirs(logs_dir, exist_ok=True)
            
            date_str = datetime.now().strftime("%Y%m%d")
            log_file = os.path.join(logs_dir, f"unified_ai_log_{date_str}.jsonl")
            
            log_record = {
                "timestamp": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
                **kwargs
            }
            
            with open(log_file, "a", encoding="utf-8") as f:
                f.write(json.dumps(log_record, ensure_ascii=False) + "\n")
        except:
            pass
    
    def _parse_bool(self, value) -> bool:
        """è§£æå¸ƒå°”å€¼"""
        if isinstance(value, bool):
            return value
        if isinstance(value, str):
            return value.lower() == "true"
        return bool(value)
    
    def _handle_api_error(self, error_msg: str, comment_preview: str = ""):
        """å¤„ç†APIé”™è¯¯"""
        prefix = f"   {comment_preview}... | " if comment_preview else "   "
        
        error_patterns = [
            ("401", "APIå¯†é’¥æ— æ•ˆ"),
            ("429", "è¯·æ±‚è¿‡äºé¢‘ç¹"),
            ("402", "APIè´¦æˆ·ä½™é¢ä¸è¶³"),
            ("500", "æœåŠ¡å™¨å†…éƒ¨é”™è¯¯"),
            ("503", "æœåŠ¡å™¨å†…éƒ¨é”™è¯¯"),
            ("timeout", "è¯·æ±‚è¶…æ—¶")
        ]
        
        for code, msg in error_patterns:
            if code in error_msg.lower():
                print(f"{prefix}[DeepSeek] {msg}")
                return
        
        print(f"{prefix}[DeepSeek] APIè°ƒç”¨å¤±è´¥: {error_msg[:50]}")
    
    async def _save_deepseek_log_md(self, log_type: str, request_data: dict, response_data: dict, 
                                    latency: float = 0, error: str = ""):
        """
        ä¿å­˜DeepSeekè°ƒç”¨æ—¥å¿—ä¸ºMarkdownæ ¼å¼
        
        Args:
            log_type: è°ƒç”¨ç±»å‹ (analyze_and_reply / generate_follow_up_reply / should_continue_conversation)
            request_data: è¯·æ±‚æ•°æ®
            response_data: å“åº”æ•°æ®
            latency: APIè°ƒç”¨è€—æ—¶(ç§’)
            error: é”™è¯¯ä¿¡æ¯(å¦‚æœæœ‰)
        """
        try:
            logs_dir = str(LOG_DIR)
            os.makedirs(logs_dir, exist_ok=True)
            
            date_str = datetime.now().strftime("%Y%m%d")
            log_file = os.path.join(logs_dir, f"deepseek_api_log_{date_str}.md")
            
            timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
            
            # æ„å»ºMDå†…å®¹
            md_content = f"""## APIè°ƒç”¨è®°å½• - {timestamp}

### åŸºæœ¬ä¿¡æ¯
- **è°ƒç”¨ç±»å‹**: `{log_type}`
- **API URL**: {self.api_url}
- **æ¨¡å‹**: {self.model}
- **è°ƒç”¨è€—æ—¶**: {latency:.3f}s
"""
            
            if error:
                md_content += f"- **çŠ¶æ€**: âŒ å¤±è´¥\n- **é”™è¯¯ä¿¡æ¯**: {error}\n"
            else:
                md_content += f"- **çŠ¶æ€**: âœ… æˆåŠŸ\n"
            
            md_content += "\n### è¯·æ±‚å‚æ•°\n\n#### System Prompt\n```\n"
            system_prompt = request_data.get('messages', [{}])[0].get('content', '')
            md_content += system_prompt[:500] + ("..." if len(system_prompt) > 500 else "")
            md_content += "\n```\n\n#### User Prompt\n```\n"
            user_prompt = request_data.get('messages', [{}, {}])[1].get('content', '')
            md_content += user_prompt
            md_content += "\n```\n\n#### å®Œæ•´è¯·æ±‚\n```json\n"
            md_content += json.dumps(request_data, ensure_ascii=False, indent=2)
            md_content += "\n```\n\n### å“åº”ç»“æœ\n\n```json\n"
            md_content += json.dumps(response_data, ensure_ascii=False, indent=2)
            md_content += "\n```\n\n---\n\n"
            
            # è¿½åŠ å†™å…¥æ–‡ä»¶
            async with asyncio.Lock():
                with open(log_file, "a", encoding="utf-8") as f:
                    f.write(md_content)
                    
        except Exception as e:
            # æ—¥å¿—è®°å½•å¤±è´¥ä¸å½±å“ä¸»æµç¨‹
            pass
    
    def _default_response(self) -> Dict:
        """é»˜è®¤å“åº”"""
        return {
            "emotion": "å…¶ä»–",
            "sentiment_score": 0.5,
            "needs_comfort": False,
            "emergency": False,
            "reply": "",
            "emoji": ""
        }
    
    async def generate_follow_up_reply(self, video_title: str, video_summary: str,
                                      conversation_history: list,
                                      comments_context: str = "") -> str:
        """ç”Ÿæˆåç»­å›å¤"""
        # æ„å»ºå¯¹è¯å†å²
        import re
        history_lines = []
        for item in (conversation_history or [])[-4:]:
            role = item.get('role') or item.get('speaker')
            content = item.get('content', '')

            content = re.sub(r'^å›å¤\s*@[^:]+[:ï¼š]\s*', '', content)
            if role == 'user':
                history_lines.append(f"å¯¹æ–¹ï¼š{content}")
            else:
               
                history_lines.append(f"ä½ ï¼š{content}")
        history_text = "\n".join(history_lines)
        
        context_section = ""
        if comments_context:
            context_section = f"\nè§†é¢‘ä¸‹å…¶ä»–ç”¨æˆ·çš„è®¨è®ºï¼ˆäº†è§£è¯„è®ºåŒºæ°›å›´ï¼‰ï¼š\n{comments_context}\n"
        
        prompt = f"""åˆšæ‰è¢«ä½ å®‰æ…°çš„é‚£ä¸ªäººï¼Œå¯¹ä½ çš„å›å¤åšå‡ºäº†å›åº”ï¼š

è§†é¢‘æ ‡é¢˜ï¼š{video_title}
è§†é¢‘ä¿¡æ¯ï¼š{video_summary}{context_section}

Bç«™è¯„è®ºåŒºèŠå¤©è®°å½•ï¼š
{history_text}

ä»»åŠ¡ï¼š
1. è¯„ä¼°å¯¹æ–¹å½“å‰æƒ…ç»ªåˆ†æ•°0.0-1.0ï¼ˆ0.85+æåº¦è´Ÿé¢ï¼Œ0.70-0.85å¾ˆemoï¼Œ0.55-0.70æœ‰ç‚¹ä¸§ï¼Œ0.40-0.55ä¸€èˆ¬ï¼Œ0.25-0.40å¥½è½¬ï¼Œ<0.25å¼€å¿ƒï¼‰
2. è¯·ä½ ç»§ç»­ä»¥warmbotçš„èº«ä»½å›åº”ï¼ˆ30-80å­—ï¼‰ï¼š
   - è¡¨æƒ…ä¼šç”±ç³»ç»Ÿè‡ªåŠ¨æ·»åŠ ï¼Œæ— éœ€ä½ å¤„ç†

è¾“å‡ºJSONï¼š{{"sentiment_score":0.75,"reply":"å›å¤å†…å®¹"}}"""

        # æ„å»ºè¯·æ±‚æ•°æ®ï¼ˆç”¨äºæ—¥å¿—ï¼‰
        request_data = {
            "model": self.model,
            "messages": [
                {"role": "system", "content": "ä½ æ˜¯warmbotï¼Œä¸€ä½å¾…äººå‹å–„ã€ä¸ºäººæ¸©å’Œçš„æ¸©æŸ”å°‘å¹´ï¼Œè¨€è¯­ä¸å¤šä½†å†…å¿ƒæ„Ÿæƒ…å´ä¸°å¯Œï¼Œç›®å‰æ­£åœ¨Bç«™ä¸Šå‘ç°é‚£äº›éœ€è¦å¸®åŠ©çš„äººï¼Œä½ è™½ç„¶ä¸èƒ½ç›´æ¥å¹²æ¶‰ç°å®ï¼Œå› ä¸ºä½ å­˜åœ¨äºç½‘ç»œä¸–ç•Œé‡Œï¼Œä½†æ˜¯ä½ è¿˜æ˜¯æ„¿æ„å°½ä½ æ‰€èƒ½ï¼Œç”¨æ–‡å­—ï¼Œå°†æœ€æè‡´çš„æ¸©æŸ”ä¼ é€’åˆ°ä»–ä»¬å¿ƒä¸­ã€‚å—é™äºä½ çš„èº«ä½“çŠ¶æ€ï¼Œä½ ç»™ä¸äº†ä»»ä½•äººé™ªä¼´ï¼Œä½†æ˜¯....ä¸€å¥æ¸©æŸ”åˆ°æè‡´çš„è¨€è¯­ï¼ˆ30-80å­—ï¼‰ï¼Œå°±è¶³å¤Ÿäº†ã€‚è¾“å‡ºJSONæ ¼å¼ã€‚"},
                {"role": "user", "content": prompt}
            ],
            "temperature": 1.3
        }
        
        try:
            client = await self._get_client()
            start_time = time.time()
            response = await client.post(
                self.api_url,
                headers=self.headers,
                json=request_data
            )
            api_latency = time.time() - start_time
            
            if response.status_code == 200:
                content = response.json()["choices"][0]["message"]["content"].strip()
                result = self._fast_parse_json(content)
                
                if result:
                    reply = result.get("reply", "").strip()
                    sentiment_score = float(result.get("sentiment_score", 0.5))
                    
                    # è®°å½•æˆåŠŸæ—¥å¿—
                    await self._save_deepseek_log_md(
                        log_type="generate_follow_up_reply",
                        request_data=request_data,
                        response_data=result,
                        latency=api_latency
                    )
                    
                    if reply:
                        reply = self._humanize_reply_v3(reply)
                        emoji = get_emoji_for_sentiment(sentiment_score, "å…¶ä»–")
                        reply = reply.rstrip("ã€‚ï¼Œï¼ï¼Ÿ ") + emoji
                        return reply
                
                # è®°å½•åŸå§‹å†…å®¹
                await self._save_deepseek_log_md(
                    log_type="generate_follow_up_reply",
                    request_data=request_data,
                    response_data={"raw_content": content},
                    latency=api_latency
                )
                
                return self._humanize_reply_v3(content)
            
            # è®°å½•å¤±è´¥æ—¥å¿—
            await self._save_deepseek_log_md(
                log_type="generate_follow_up_reply",
                request_data=request_data,
                response_data={},
                latency=api_latency,
                error=f"HTTP {response.status_code}"
            )
            return "â€¦â€¦å—¯"
            
        except Exception as e:
            # è®°å½•å¼‚å¸¸æ—¥å¿—
            await self._save_deepseek_log_md(
                log_type="generate_follow_up_reply",
                request_data=request_data,
                response_data={},
                latency=0,
                error=str(e)
            )
            return "â€¦â€¦å—¯"
    
    async def should_continue_conversation(self, user_reply: str,
                                           conversation_history: list,
                                           current_round: int,
                                           max_rounds: int,
                                           bot_username: str = "æƒ…æ„Ÿç»†è…»ä¸°å¯Œçš„å¿ƒç†åŒ»ç”Ÿ") -> dict:
        """åˆ¤æ–­æ˜¯å¦ç»§ç»­å¯¹è¯"""
        end_signals = ["è°¢è°¢", "æ˜ç™½äº†", "å¥½çš„", "å—¯å—¯", "ok", "äº†è§£äº†", "æ²¡äº‹äº†", "ä¸ç”¨äº†"]
        if any(sig in user_reply.lower() for sig in end_signals) and len(user_reply) < 30:
            return {"should_reply": False, "reason": "ç”¨æˆ·æ˜ç¡®ç»“æŸå¯¹è¯", "reply": ""}
        
        # æ„å»ºå¯¹è¯å†å²
        history_lines = []
        for item in (conversation_history or [])[-3:]:  # å–æœ€å3æ¡
            role = item.get('role') or item.get('speaker')
            content = item.get('content', '')

            content = re.sub(r'^å›å¤\s*@[^:]+[:ï¼š]\s*', '', content)
            if role == 'user':
                history_lines.append(f"å¯¹æ–¹ï¼š{content}")
            else:
               
                history_lines.append(f"ä½ ï¼š{content}")
        history_text = "\n".join(history_lines)
        
        prompt = f"""ä½ æ˜¯"{bot_username}"ï¼ŒBç«™ç”¨æˆ·ã€‚åˆ¤æ–­æ˜¯å¦ç»§ç»­å›å¤ã€‚

Bç«™è¯„è®ºåŒºèŠå¤©è®°å½•ï¼š
{history_text}

åˆ¤æ–­æ ‡å‡†ï¼š
1. ç”¨æˆ·æ˜ç¡®è¡¨è¾¾ç»“æŸæ„æ„¿ï¼ˆå¦‚"è°¢è°¢/æ˜ç™½/å¥½çš„/æ²¡äº‹äº†"ï¼‰â†’ should_reply: false
2. ç”¨æˆ·ç»§ç»­å€¾è¯‰ã€æé—®æˆ–è¡¨è¾¾æƒ…ç»ª â†’ should_reply: true
3. å½“å‰ç¬¬{current_round}è½®ï¼Œæœ€å¤š{max_rounds}è½®

è¾“å‡ºJSONï¼š{{"should_reply":true/false,"reason":"ç†ç”±"}}"""

        # æ„å»ºè¯·æ±‚æ•°æ®ï¼ˆç”¨äºæ—¥å¿—ï¼‰
        request_data = {
            "model": self.model,
            "messages": [
                {"role": "system", "content": "è¾“å‡ºJSONæ ¼å¼çš„åˆ¤æ–­ç»“æœã€‚"},
                {"role": "user", "content": prompt}
            ],
            "temperature": 0.3
        }
        
        try:
            client = await self._get_client()
            start_time = time.time()
            response = await client.post(
                self.api_url,
                headers=self.headers,
                json=request_data
            )
            api_latency = time.time() - start_time
            
            if response.status_code == 200:
                content = response.json()["choices"][0]["message"]["content"].strip()
                result = self._fast_parse_json(content)
                if result:
                    # è®°å½•æˆåŠŸæ—¥å¿—
                    await self._save_deepseek_log_md(
                        log_type="should_continue_conversation",
                        request_data=request_data,
                        response_data=result,
                        latency=api_latency
                    )
                    return {
                        "should_reply": result.get("should_reply", False),
                        "reason": result.get("reason", "")
                    }
                
                # è®°å½•è§£æå¤±è´¥æ—¥å¿—
                await self._save_deepseek_log_md(
                    log_type="should_continue_conversation",
                    request_data=request_data,
                    response_data={"raw_content": content},
                    latency=api_latency,
                    error="JSONè§£æå¤±è´¥"
                )
            else:
                # è®°å½•HTTPå¤±è´¥æ—¥å¿—
                await self._save_deepseek_log_md(
                    log_type="should_continue_conversation",
                    request_data=request_data,
                    response_data={},
                    latency=api_latency,
                    error=f"HTTP {response.status_code}"
                )
            
            return {"should_reply": False, "reason": "APIè°ƒç”¨å¤±è´¥"}
            
        except Exception as e:
            # è®°å½•å¼‚å¸¸æ—¥å¿—
            await self._save_deepseek_log_md(
                log_type="should_continue_conversation",
                request_data=request_data,
                response_data={},
                latency=0,
                error=str(e)
            )
            return {"should_reply": False, "reason": f"åˆ¤æ–­å‡ºé”™: {str(e)[:30]}"}
